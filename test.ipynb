{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('captures2/2023_05_19_19_02_56.613060 n.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.resize((480, 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.PILToTensor()(img).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 480, 480])"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                      kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "\n",
    "class STEM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(STEM, self).__init__()\n",
    "        self.conv1 = ConvolutionBlock(\n",
    "            in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=0)\n",
    "        self.conv2 = ConvolutionBlock(\n",
    "            in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = ConvolutionBlock(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.conv4 = ConvolutionBlock(\n",
    "            in_channels=64, out_channels=80, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv5 = ConvolutionBlock(\n",
    "            in_channels=80, out_channels=192, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)  # 149 x 149 x 64\n",
    "        x = self.pool1(x)  # 147 x 147 x 64\n",
    "        x = self.conv4(x)  # 147 x 147 x 80\n",
    "        x = self.conv5(x)  # 147 x 147 x 192\n",
    "        x = self.pool2(x)  # 73 x 73 x 192\n",
    "        return x\n",
    "\n",
    "\n",
    "class InceptionBlockA(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionBlockA, self).__init__()\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=64,\n",
    "                             kernel_size=1, stride=1, padding=0),\n",
    "            ConvolutionBlock(in_channels=64, out_channels=96,\n",
    "                             kernel_size=3, stride=1, padding=1),\n",
    "            ConvolutionBlock(in_channels=96, out_channels=96, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=48,\n",
    "                             kernel_size=1, stride=1, padding=0),\n",
    "            ConvolutionBlock(in_channels=48, out_channels=64, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=64, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "        self.branch4 = ConvolutionBlock(\n",
    "            in_channels=in_channels, out_channels=64, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], dim=1)\n",
    "\n",
    "\n",
    "class ReductionBlockA(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ReductionBlockA, self).__init__()\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=64,\n",
    "                             kernel_size=1, stride=1, padding=0),\n",
    "            ConvolutionBlock(in_channels=64, out_channels=96,\n",
    "                             kernel_size=3, stride=1, padding=1),\n",
    "            ConvolutionBlock(in_channels=96, out_channels=96, kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=384, kernel_size=2, stride=2, padding=0))\n",
    "        \n",
    "        self.branch3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1)\n",
    "\n",
    "\n",
    "class InceptionBlockB(nn.Module):\n",
    "    def __init__(self, in_channels, nbr_kernel):\n",
    "        super(InceptionBlockB, self).__init__()\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=nbr_kernel,\n",
    "                             kernel_size=1, stride=1, padding=0),\n",
    "            ConvolutionBlock(in_channels=nbr_kernel, out_channels=nbr_kernel, kernel_size=(\n",
    "                7, 1), stride=1, padding=(0,3)),\n",
    "            ConvolutionBlock(in_channels=nbr_kernel, out_channels=nbr_kernel, kernel_size=(\n",
    "                1, 7), stride=1, padding=(3,0)),\n",
    "            ConvolutionBlock(in_channels=nbr_kernel, out_channels=nbr_kernel, kernel_size=(\n",
    "                7, 1), stride=1, padding=(0, 3)),\n",
    "            ConvolutionBlock(in_channels=nbr_kernel, out_channels=192, kernel_size=(1, 7), stride=1, padding=(3,0)))\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=nbr_kernel,\n",
    "                             kernel_size=1, stride=1, padding=0),\n",
    "            ConvolutionBlock(in_channels=nbr_kernel, out_channels=nbr_kernel, kernel_size=(\n",
    "                1, 7), stride=1, padding=(0,3)),\n",
    "            ConvolutionBlock(in_channels=nbr_kernel, out_channels=192, kernel_size=(7, 1), stride=1, padding=(3,0)))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=192, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "        self.branch4 = ConvolutionBlock(\n",
    "            in_channels=in_channels, out_channels=192, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], dim=1)\n",
    "\n",
    "\n",
    "class ReductionBlockB(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ReductionBlockB, self).__init__()\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=192,\n",
    "                             kernel_size=1, stride=1, padding=0),\n",
    "            ConvolutionBlock(in_channels=192, out_channels=192,\n",
    "                             kernel_size=(1, 7), stride=1, padding=(0, 3)),\n",
    "            ConvolutionBlock(in_channels=192, out_channels=192,\n",
    "                             kernel_size=(7, 1), stride=1, padding=(3,0)),\n",
    "            ConvolutionBlock(in_channels=192, out_channels=192, kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=192,\n",
    "                             kernel_size=1, stride=1, padding=0),\n",
    "            ConvolutionBlock(in_channels=192, out_channels=320, kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1)\n",
    "\n",
    "\n",
    "class AuxiliaryClassifierBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(AuxiliaryClassifierBlock, self).__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=5, stride=3, padding=0),\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=128,\n",
    "                             kernel_size=1, stride=1, padding=0),\n",
    "            ConvolutionBlock(in_channels=128, out_channels=768,\n",
    "                                 kernel_size=1, stride=1, padding=0),\n",
    "            )\n",
    "\n",
    "        self.fc1 = nn.LazyLinear(1024)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, 1000)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.branch1(x)\n",
    "        x = torch.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return nn.Softmax()(x)\n",
    "\n",
    "\n",
    "class InceptionBlockC(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=384, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "        self.branch1_1 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=384, out_channels=384, kernel_size=(1, 3), stride=1, padding=(0,1)))\n",
    "        \n",
    "        self.branch1_2 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=384, out_channels=384,\n",
    "                             kernel_size=(3,1), stride=1, padding=(1,0))\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=448,\n",
    "                             kernel_size=1, stride=1, padding=0),\n",
    "            ConvolutionBlock(in_channels=448, out_channels=384,kernel_size= 3, stride = 1 ,padding=1)\n",
    "        )\n",
    "        self.branch2_1 = ConvolutionBlock(\n",
    "            in_channels=384, out_channels=384, kernel_size=(1, 3), stride=1, padding=(0,1))\n",
    "        \n",
    "        self.branch2_2 = ConvolutionBlock(\n",
    "            in_channels=384, out_channels=384, kernel_size=(3, 1), stride=1, padding=(1,0))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvolutionBlock(in_channels=in_channels, out_channels=192, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        self.branch4 = ConvolutionBlock(\n",
    "            in_channels=in_channels, out_channels=320, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.branch1_1(self.branch1(x)), self.branch1_2(self.branch1(x)), self.branch2_1(self.branch2(x)), self.branch2_2(self.branch2(x)), self.branch3(x), self.branch4(x)], dim=1)\n",
    "\n",
    "\n",
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = STEM()\n",
    "\n",
    "        self.InceptionA1 = InceptionBlockA(192)\n",
    "        self.InceptionA2 = InceptionBlockA(288)\n",
    "        self.InceptionA3 = InceptionBlockA(288)\n",
    "\n",
    "        self.redA = ReductionBlockA(288)\n",
    "\n",
    "        self.InceptionB1 = InceptionBlockB(768, 128)\n",
    "        self.InceptionB2 = InceptionBlockB(768,160)\n",
    "        self.InceptionB3 = InceptionBlockB(768,160)\n",
    "        self.InceptionB4 = InceptionBlockB(768,192)\n",
    "\n",
    "        self.redB = ReductionBlockB(768)\n",
    "\n",
    "\n",
    "        self.InceptionC1 = InceptionBlockC(1280)\n",
    "        self.InceptionC2 = InceptionBlockC(2048)\n",
    "\n",
    "        self.aux = AuxiliaryClassifierBlock(768)\n",
    "\n",
    "        self.pool = nn.AvgPool2d(kernel_size = 1)\n",
    "        self.fc1 = nn.LazyLinear( 1000)\n",
    "\n",
    "        self.fc2 = nn.Linear(1000, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.stem(x)\n",
    "\n",
    "        x = self.InceptionA1(x)\n",
    "        x = self.InceptionA2(x)\n",
    "        x = self.InceptionA2(x)\n",
    "\n",
    "        x = self.redA(x)\n",
    "\n",
    "        x = self.InceptionB1(x)\n",
    "        x = self.InceptionB2(x)\n",
    "        x = self.InceptionB3(x)\n",
    "        x = self.InceptionB4(x)\n",
    "\n",
    "        aux = self.aux(x)\n",
    "\n",
    "        x = self.redB(x)\n",
    "\n",
    "        x = self.InceptionC1(x)\n",
    "        x = self.InceptionC2(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataset import DinoDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import CenterCrop, Resize, Compose, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "transformer = Compose([\n",
    "    Resize((480, 480)),\n",
    "    CenterCrop(480),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_frame = pd.read_csv('labels_dino.csv')\n",
    "train, test = train_test_split(key_frame, test_size=0.2)\n",
    "\n",
    "train = pd.DataFrame(train)\n",
    "test = pd.DataFrame(test)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = DinoDataset(root_dir=\"captures2\",\n",
    "                       dataframe=train, transform=transformer)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size)\n",
    "\n",
    "testset = DinoDataset(root_dir=\"captures2\",\n",
    "                      dataframe=test, transform=transformer)\n",
    "testloader = DataLoader(testset, batch_size=batch_size)\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = InceptionV3()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.009)\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    epochs = 15  # number of training passes over the mini batches\n",
    "    loss_container = []  # container to store the loss values after each epoch\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for data in tqdm(trainloader, position=0, leave=True):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        loss_container.append(running_loss)\n",
    "\n",
    "        print(f'[{epoch + 1}] | loss: {running_loss / len(trainloader):.3f}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    torch.save(model.state_dict(), 'path_to_save_model.pth')\n",
    "\n",
    "    # plot the loss curve\n",
    "    plt.plot(np.linspace(1, epochs, epochs).astype(int), loss_container)\n",
    "    plt.show()\n",
    "\n",
    "    # clean up the gpu memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1043 [00:00<?, ?it/s]/var/folders/sl/6pfddh5x4ljcbvrtvkcnqglc0000gn/T/ipykernel_1623/2110645663.py:173: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.Softmax()(x)\n",
      " 56%|█████▌    | 583/1043 [28:47<22:43,  2.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(model, criterion, optimizer, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[118], line 23\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     24\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     25\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[115], line 257\u001b[0m, in \u001b[0;36mInceptionV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    255\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mInceptionB1(x)\n\u001b[1;32m    256\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mInceptionB2(x)\n\u001b[0;32m--> 257\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mInceptionB3(x)\n\u001b[1;32m    258\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mInceptionB4(x)\n\u001b[1;32m    260\u001b[0m aux \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maux(x)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[115], line 123\u001b[0m, in \u001b[0;36mInceptionBlockB.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbranch1(x), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbranch2(x), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbranch3(x), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbranch4(x)], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[115], line 15\u001b[0m, in \u001b[0;36mConvolutionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_block(x)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, criterion, optimizer, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in trainloader:\n",
    "    a = x\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = InceptionV3().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/6pfddh5x4ljcbvrtvkcnqglc0000gn/T/ipykernel_1623/3699613788.py:173: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.Softmax()(x)\n",
      "/var/folders/sl/6pfddh5x4ljcbvrtvkcnqglc0000gn/T/ipykernel_1623/3699613788.py:272: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4536, 0.5464],\n",
       "        [0.5021, 0.4979],\n",
       "        [0.5219, 0.4781],\n",
       "        [0.5793, 0.4207]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc(a.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
